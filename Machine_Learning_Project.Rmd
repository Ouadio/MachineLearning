---
title: "Machine Learning to Monitor Exercise Quality"
output: 
  html_document:
    keep_md: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

Kristin Abkemeier, January 2018
===============================

## Synopsis

This assignment entails analyzing data from accelerometers placed on the belt, forearm, upper arm, and dumbbell of 6 people who performed dumbbell lifts correctly and then incorrectly in five different specific ways. The dumbbell lift motions were categorized into classes A, B, C, D, or E based on how the subject performed the exercise. I sought to find a machine learning algorithm that could be trained on the properly cleaned data so that the algorithm could correctly identify how the dumbbell lift was done according to class (called "classe" in this data set). After paring down the given data set to 44 more-or-less independent columns and exploring some different machine learning approaches, it was possible to achieve 99+ percent accuracy with a random forest approach.

## Exploring and Cleaning the Data

For this report, I read in the data contained in the file pml-training.csv that was collected in the research by [Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; and Fuks, H., "Qualitative Activity Recognition of Weight Lifting Exercises, 2013](http://web.archive.org/web/20170519033209/http://groupware.les.inf.puc-rio.br:80/public/papers/2013.Velloso.QAR-WLE.pdf). I also set a seed value for reproducibility and read in a couple of libraries that I found that I would need during the course of my investigations.

```{r, echo=TRUE, quietly=TRUE, warning = FALSE, message=FALSE}
## Loading and preprocessing the data
setwd("/Users/kristinabkemeier/DataScience/Practical Machine Learning/Project")
actData <- read.csv("pml-training.csv", header=TRUE)
## Set seed for reproducibility
set.seed(4326)
## And read in a couple of the libraries that we will need to use
library(caret)
library(e1071)
```

I needed to partition the training data set so that we could test our model before we tried testing it on the 20 testing values given in the file pml-testing.csv. I chose to put 75% of the pml-training.csv data into my training set and 25% into my testing set for checking my out of sample error rate.
```{r, echo=TRUE}
inTrain <- createDataPartition(y=actData$X, p=0.75, list=FALSE)
training <- actData[inTrain,]
testing <- actData[-inTrain,]
```

Because I started with 160 columns in the table, I did an initial visual inspection of the table to get rid of the columns that would not contribute towards a good fit of the data. It was cleqr that some columns were very sparsely populated, with only occasional nonzero entries that contained quantities that were averaged over multiple rows over time. It was clear from looking at the pml-testing.csv data set to be used for the final test that we were not expected to use any time-averaged results, so I got rid of the sparse columns, which we wouldn't be able to impute values for anyway. Also, the first seven columns contained names of the subjects, time stamps, and indicators for the block of rows that would get averaged over for entry into the sparsely-populated columns. 
```{r, echo=TRUE}
colsToRemove <-c(1:7,12:36,50:59,69:83,87:101,103:112,125:139,141:150)

aData <- training[,-colsToRemove]
aDataTest <- testing[,-colsToRemove]
```

The 53 columns of numerical data that I was left with was already in a usable format and did not need to be converted into a different variable type. So I began to make some exploratory plots, color-coding by classe type to see if I could discern any trends. Here is one such example:
```{r, echo=TRUE}
qplot(aData$pitch_belt, aData$roll_arm, col=aData$classe)
```

Between six different subjects, each of whom had a different manner of conducting the exercise, I could see no clear trend anywhere. There only seemed to be clusters of points, overlapping everywhere! I had to trust that the machine learning tools would work, because no linear regression model - or any kind of regression model - was going to be able to tease out a trend from this mess.

Before I invoked some machine learning algorithms, I did apply one lesson from regression modeling: pare away any highly-correlated regressors, so that we attempt to fit our model on a more independent basis of regressors. So, I created a correlation matrix to see if we had any strongly correlated columns:
```{r, echo=TRUE}
## Remove the classe column for this
M <-abs(cor(aData[,-53]))
diag(M) <- 0
## Looking for correlations greater than 0.80 yields 19 distinct pairs, but no surprises
wc <- which(M > 0.8, arr.ind=T) 
wc
```

As a result of what I discovered in the correlation matrix, I decided to remove the duplicative columns from my training data set, and the testing data set that I had set aside:
```{r, echo=TRUE}
corrColsToRemove <-c(3,4,8,9,10,11,19,24,26,33,36)
aData2 <- aData[,-corrColsToRemove]
aDataTest2 <- aDataTest[,-corrColsToRemove]
```

## Machine Learning Attempts

I tried a handful of different machine learning algorithms from the caret package. First, I tried rpart, for partitioning the data into a tree. It performed abominably, only yielding about three branches and giving up almost entirely on sorting classes C and D. But all it does is separate data into groups, with no attempt at additional modeling.

Next, I tried train with the "treebag" model, which is a Bagged CART approach. It yielded an answer pretty quickly after just a couple of minutes of straightforward, non-parallel processing. When I checked my fit on my test data, I had upwards of 98 percent accuracy, or less than 2 percent out of sample error, which was encouraging:
```{r, echo=TRUE}
modelFit <- train(classe ~.,data=aData2, method="treebag")
predictions <- predict(modelFit, newdata = aDataTest2)
confusionMatrix(predictions, aDataTest2$classe)
```

However good 98% sounded, it wasn't good enough. I also tried train() using "bagEarth" and "bagFDA" models, but they ran without yielding any result. This was before I learned about setting up parallel processing from Coursera mentor Leonard Greski's [notes](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) on GitHub and tried a random forest approach, using 5-fold cross-validation:
```{r, echo=TRUE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

fitControl2 <- trainControl(
  method = "cv",
  number = 5,
  allowParallel = TRUE)

modelFit <- train(classe ~.,data=aData2, method="rf", trControl = fitControl2)
## This only took a few minutes, once I switched on the parallel processing!
## So now we shut off the parallel processing
stopCluster(cluster)
registerDoSEQ()
predictions <- predict(modelFit, newdata = aDataTest2)
confusionMatrix(predictions, aDataTest2$classe)
```

The accuracy of the random forest exceeded 99 percent, meaning my out of sample error was less than one percent. There was cross-validation in the model (trainControl() used method "cv"), and armed with the knowledge that I had such great accuracy, I made predictions from pml-testing.csv from this model and got 100 percent correct answers, according to the quiz. Note that I had to process this data set the same way that I tested the validation data that I had split off from the training set.

```{r, echo=TRUE}
testData <- read.csv("pml-testing.csv", header=TRUE)
aDataCourseraTest <- testData[,-colsToRemove]
aDataCourseraTest2 <- aDataCourseraTest[,-corrColsToRemove]
predictions <- predict(modelFit, newdata = aDataCourseraTest2)
```

Finally, I decided to try the boosting method gbm to see if it would do better than random forest. Unfortunately, it had the lowest accuracy of the three methods. Initially I couldn't even get the boosting to converge, but then I applied parallel computing processes, and again, I got a result.

```{r, echo=TRUE}
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
gbmFit1 <- train(classe ~ ., data = aData2, 
                 method = "gbm", 
                 trControl = fitControl2, verbose = FALSE)
stopCluster(cluster)
registerDoSEQ()
predictions <- predict(gbmFit1, newdata = aDataTest2)
confusionMatrix(predictions, aDataTest2$classe)
```

However, at around 95 percent accuracy, this boosting algorithm does not work as well as the random forest approach.

## Conclusion

Several different machine learning approaches were used to find which was most effective for predicting the class of performing the dumbbell lift exercise. The random forest algorithm was up to the task for the challenges of this particular data set.
